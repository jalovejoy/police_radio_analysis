{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Police Radio Analysis: Data Collection (using Local Library)\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "- [Overview](#Overview)\n",
    "- [Dataframe by Word](#Data-Collection:-Single-Word-Observation)\n",
    "- [Dataframe by Sentence](#Data-Collection:-Sentence-by-Speaker-Observation)\n",
    "- [Dataframe by Location (for Mapping)](#Data-Collection:-Threatened-Locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Audio Files: Broadcastify archives for 11/08/2018 between 6:44AM-9:12AM in Butte County, CA (The Camp Fire)\n",
    "\n",
    "\n",
    "- Used Amazon Transcribe to transcribe audio to text (output JSON file)\n",
    "\n",
    "\n",
    "- Steps to convert JSON files to a structured dataframes:\n",
    "    1. Take word and speaker information from individual dataframe to create two dataframes\n",
    "    2. Merge the two dataframes by assigning the speaker to the word based on the time\n",
    "    3. Based on speaker and start/end times reconstruct sentences (new speaker signifies new observation)\n",
    "    4. Repeat for every JSON file and combine all into on dataframe\n",
    "    5. Add additional desired columns based on current information in dataframe:\n",
    "        - Clean version of text (no punctuation, all lowercase)\n",
    "        - Start and stop time as datetime objects using the initial start time in the file name\n",
    "        - Indicator for containing 'fire' or 'evacuation' in the observations\n",
    "    6. Sort dataframe by the start time (datetime object) and export\n",
    "    7. Use existing dataframe and to make a new dataframe where each identified location is one observation\n",
    "\n",
    "\n",
    "- Output: 3 Dataframes\n",
    "    - Dataframe by Individual Words\n",
    "    - Dataframe by Sentences\n",
    "    - Dataframe by Locations Mentioned (for Mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import geopandas as gpd\n",
    "\n",
    "# Local Library\n",
    "import data_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function data_collection.get_dataframe(file_name, ca_places)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collection.get_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Geographic Data for Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads geographic data from https://data.ca.gov/dataset/ca-geographic-boundaries\n",
    "ca_places = gpd.read_file('./data/CA_places/CA_Places_TIGER2016.shp')\n",
    "ca_state = gpd.read_file('./data/CA_State/CA_State_TIGER2016.shp')\n",
    "ca_counties = gpd.read_file('./data/CA_Counties/CA_Counties_TIGER2016.shp')\n",
    "\n",
    "## Reset INTPTLAT & INTPTLON to floats\n",
    "ca_places['INTPTLAT'] = ca_places['INTPTLAT'].astype('float')\n",
    "ca_places['INTPTLON'] = ca_places['INTPTLON'].astype('float')\n",
    "ca_state['INTPTLAT'] = ca_state['INTPTLAT'].astype('float')\n",
    "ca_state['INTPTLON'] = ca_state['INTPTLON'].astype('float')\n",
    "ca_counties['INTPTLAT'] = ca_counties['INTPTLAT'].astype('float')\n",
    "ca_counties['INTPTLON'] = ca_counties['INTPTLON'].astype('float')\n",
    "\n",
    "## Reset places dataframe to capital NAME\n",
    "for i, row in enumerate(ca_places.iterrows()):\n",
    "    ca_places.loc[i,'NAME'] = row[1]['NAME'].upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection: Single Word Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 20 Dataframes Completed (558 rows): ./data/translations/201811081001-444704-22956_.json\n",
      "2 of 20 Dataframes Completed (1146 rows): ./data/translations/201811080929-467022-25641_.json\n",
      "3 of 20 Dataframes Completed (1201 rows): ./data/translations/201811080858-659667-24574_.json\n",
      "4 of 20 Dataframes Completed (2511 rows): ./data/translations/201811080931-763045-22956_.json\n",
      "5 of 20 Dataframes Completed (3921 rows): ./data/translations/201811080901-584135-22956_.json\n",
      "6 of 20 Dataframes Completed (4113 rows): ./data/translations/201811081011-319947-26936_.json\n",
      "7 of 20 Dataframes Completed (4253 rows): ./data/translations/201811080911-136992-26936_.json\n",
      "8 of 20 Dataframes Completed (5080 rows): ./data/translations/201811081012-237044-1929_.json\n",
      "9 of 20 Dataframes Completed (5143 rows): ./data/translations/201811080959-402082-25641_.json\n",
      "10 of 20 Dataframes Completed (5208 rows): ./data/translations/201811080928-650127-24574_.json\n",
      "11 of 20 Dataframes Completed (5637 rows): ./data/translations/201811081031-942894-22956_.json\n",
      "12 of 20 Dataframes Completed (6349 rows): ./data/translations/201811080942-79066-1929_.json\n",
      "13 of 20 Dataframes Completed (6562 rows): ./data/translations/201811081027-123435-24574_.json\n",
      "14 of 20 Dataframes Completed (7440 rows): ./data/translations/201811081042-434498-1929_.json\n",
      "15 of 20 Dataframes Completed (7665 rows): ./data/translations/201811080841-581016-26936_.json\n",
      "16 of 20 Dataframes Completed (8229 rows): ./data/translations/201811081029-313400-25641_.json\n",
      "17 of 20 Dataframes Completed (8313 rows): ./data/translations/201811080941-981417-26936_.json\n",
      "18 of 20 Dataframes Completed (9132 rows): ./data/translations/201811080913-878163-1929_.json\n",
      "19 of 20 Dataframes Completed (9257 rows): ./data/translations/201811080958-305482-24574_.json\n",
      "20 of 20 Dataframes Completed (9644 rows): ./data/translations/201811080900-205450-25641_.json\n",
      "Words Dataframe Shape: (9644, 6)\n"
     ]
    }
   ],
   "source": [
    "# Get word dataframe\n",
    "\n",
    "# Use glob to get list of all json files in the folder\n",
    "files_json = (glob.glob('./data/translations/*.json'))\n",
    "\n",
    "# Create empty dataframe\n",
    "words = pd.DataFrame()\n",
    "rows = 0 \n",
    "\n",
    "for i, file_name in enumerate(files_json):\n",
    "    word_one_file, _ = data_collection.transcription_outputs(file_name)\n",
    "    \n",
    "    rows += len(word_one_file)\n",
    "    \n",
    "    # Print status\n",
    "    print(f'{i+1} of {len(files_json)} Dataframes Completed ({rows} rows): {file_name}')\n",
    "    \n",
    "    # Add each dataframe together\n",
    "    words = pd.concat([words, word_one_file])\n",
    "    \n",
    "# Reset index of master dataframe\n",
    "words.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print shape of dataframe after going through all JSON files\n",
    "print(f'Words Dataframe Shape: {words.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>confidence</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>type</th>\n",
       "      <th>feed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wait</td>\n",
       "      <td>0.3086</td>\n",
       "      <td>38.04</td>\n",
       "      <td>39.28</td>\n",
       "      <td>pronunciation</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FIRE</td>\n",
       "      <td>0.9876</td>\n",
       "      <td>39.29</td>\n",
       "      <td>39.88</td>\n",
       "      <td>pronunciation</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>with</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.88</td>\n",
       "      <td>40.64</td>\n",
       "      <td>pronunciation</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>every</td>\n",
       "      <td>0.1804</td>\n",
       "      <td>41.22</td>\n",
       "      <td>41.41</td>\n",
       "      <td>pronunciation</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>night</td>\n",
       "      <td>0.2812</td>\n",
       "      <td>41.41</td>\n",
       "      <td>41.72</td>\n",
       "      <td>pronunciation</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  content confidence start_time end_time           type  \\\n",
       "0    wait     0.3086      38.04    39.28  pronunciation   \n",
       "1    FIRE     0.9876      39.29    39.88  pronunciation   \n",
       "2    with        1.0      39.88    40.64  pronunciation   \n",
       "3   every     0.1804      41.22    41.41  pronunciation   \n",
       "4   night     0.2812      41.41    41.72  pronunciation   \n",
       "\n",
       "                         feed  \n",
       "0  201811081001-444704-22956_  \n",
       "1  201811081001-444704-22956_  \n",
       "2  201811081001-444704-22956_  \n",
       "3  201811081001-444704-22956_  \n",
       "4  201811081001-444704-22956_  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save words dataframe\n",
    "\n",
    "# Export: save as csv\n",
    "words.to_csv('./data/words.csv', index=False)\n",
    "\n",
    "# Export: save as pkl\n",
    "words.to_pickle('./data/words.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection: Sentence by Speaker Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 20 Dataframes Completed (new rows: 52, total rows: 52): ./data/translations/201811081001-444704-22956_.json\n",
      "2 of 20 Dataframes Completed (new rows: 44, total rows: 96): ./data/translations/201811080929-467022-25641_.json\n",
      "3 of 20 Dataframes Completed (new rows: 6, total rows: 102): ./data/translations/201811080858-659667-24574_.json\n",
      "4 of 20 Dataframes Completed (new rows: 50, total rows: 152): ./data/translations/201811080931-763045-22956_.json\n",
      "5 of 20 Dataframes Completed (new rows: 30, total rows: 182): ./data/translations/201811080901-584135-22956_.json\n",
      "6 of 20 Dataframes Completed (new rows: 10, total rows: 192): ./data/translations/201811081011-319947-26936_.json\n",
      "7 of 20 Dataframes Completed (new rows: 7, total rows: 199): ./data/translations/201811080911-136992-26936_.json\n",
      "8 of 20 Dataframes Completed (new rows: 45, total rows: 244): ./data/translations/201811081012-237044-1929_.json\n",
      "9 of 20 Dataframes Completed (new rows: 2, total rows: 246): ./data/translations/201811080959-402082-25641_.json\n",
      "10 of 20 Dataframes Completed (new rows: 3, total rows: 249): ./data/translations/201811080928-650127-24574_.json\n",
      "11 of 20 Dataframes Completed (new rows: 36, total rows: 285): ./data/translations/201811081031-942894-22956_.json\n",
      "12 of 20 Dataframes Completed (new rows: 21, total rows: 306): ./data/translations/201811080942-79066-1929_.json\n",
      "13 of 20 Dataframes Completed (new rows: 20, total rows: 326): ./data/translations/201811081027-123435-24574_.json\n",
      "14 of 20 Dataframes Completed (new rows: 30, total rows: 356): ./data/translations/201811081042-434498-1929_.json\n",
      "15 of 20 Dataframes Completed (new rows: 13, total rows: 369): ./data/translations/201811080841-581016-26936_.json\n",
      "16 of 20 Dataframes Completed (new rows: 21, total rows: 390): ./data/translations/201811081029-313400-25641_.json\n",
      "17 of 20 Dataframes Completed (new rows: 3, total rows: 393): ./data/translations/201811080941-981417-26936_.json\n",
      "18 of 20 Dataframes Completed (new rows: 38, total rows: 431): ./data/translations/201811080913-878163-1929_.json\n",
      "19 of 20 Dataframes Completed (new rows: 16, total rows: 447): ./data/translations/201811080958-305482-24574_.json\n",
      "20 of 20 Dataframes Completed (new rows: 17, total rows: 464): ./data/translations/201811080900-205450-25641_.json\n",
      "Sentence Dataframe Shape: (464, 17)\n"
     ]
    }
   ],
   "source": [
    "# Get sentence dataframe\n",
    "\n",
    "# Use glob to get list of all json files in the folder\n",
    "files_json = (glob.glob('./data/translations/*.json'))\n",
    "\n",
    "# Create empty dataframe\n",
    "sentence = pd.DataFrame()\n",
    "rows = 0 \n",
    "\n",
    "# Iterate through the files and get a dataframe for each file\n",
    "for i, file_name in enumerate(files_json):\n",
    "    \n",
    "    # Get dataframe for an individual file\n",
    "    df_one_file = data_collection.get_dataframe(file_name, ca_places)\n",
    "    rows += len(df_one_file)\n",
    "    \n",
    "    # Print status\n",
    "    print(f'{i+1} of {len(files_json)} Dataframes Completed (new rows: {len(df_one_file)}, total rows: {rows}): {file_name}')\n",
    "    \n",
    "    # Add each dataframe together\n",
    "    sentence = pd.concat([sentence, df_one_file])\n",
    "\n",
    "# Reset index of master dataframe\n",
    "sentence.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print shape of dataframe after going through all JSON files\n",
    "print(f'Sentence Dataframe Shape: {sentence.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Additional Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for clean text (remove punctuation and make all lowercase)\n",
    "# Reference: Code adapted from NLP_EDA-InClass in DEN Flex by Sam Stack\n",
    "def clean_text(raw_text):\n",
    "    words = re.sub(r'[^a-z0-9]', r' ', raw_text.lower()).split()\n",
    "    return ' '.join(words)\n",
    "\n",
    "sentence['text_clean'] = sentence['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for start and end time as datetime objects (using speaker_start/end and file name)\n",
    "start_time = []\n",
    "end_time = []\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    start_time.append(data_collection.actual_time_str(sentence['speaker_start'][i], sentence['feed'][i]))\n",
    "    end_time.append(data_collection.actual_time_str(sentence['speaker_end'][i], sentence['feed'][i]))\n",
    "\n",
    "sentence['start_time'] = start_time\n",
    "sentence['end_time'] = end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns that indicate if fire or evacuation related words were mentioned in that observation\n",
    "sentence['contains_fire'] = sentence['text_clean'].map(lambda x: 1 if 'fire' in x else 0)\n",
    "sentence['contains_evac'] = sentence['text_clean'].map(lambda x: 1 if 'evac' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>speaker_start</th>\n",
       "      <th>speaker_end</th>\n",
       "      <th>speaker_length</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_confidence</th>\n",
       "      <th>avg_confidence</th>\n",
       "      <th>min_conf</th>\n",
       "      <th>feed</th>\n",
       "      <th>...</th>\n",
       "      <th>feed_name</th>\n",
       "      <th>department</th>\n",
       "      <th>INTPTLON</th>\n",
       "      <th>INTPTLAT</th>\n",
       "      <th>ID_PLACES</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>contains_fire</th>\n",
       "      <th>contains_evac</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wait FIRE with every night .</td>\n",
       "      <td>52.94</td>\n",
       "      <td>53.39</td>\n",
       "      <td>0.45</td>\n",
       "      <td>spk_0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.3086, 0.9876, 1.0, 0.1804, 0.2812, nan]</td>\n",
       "      <td>0.55156</td>\n",
       "      <td>0.1804</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "      <td>...</td>\n",
       "      <td>Chico_Paradise_Fire__CalFire</td>\n",
       "      <td>FIRE</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>wait fire with every night</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-11-08 08:01:52-08:00</td>\n",
       "      <td>2018-11-08 08:01:53-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thirty</td>\n",
       "      <td>53.39</td>\n",
       "      <td>53.7</td>\n",
       "      <td>0.31</td>\n",
       "      <td>spk_2</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.6417]</td>\n",
       "      <td>0.64170</td>\n",
       "      <td>0.6417</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "      <td>...</td>\n",
       "      <td>Chico_Paradise_Fire__CalFire</td>\n",
       "      <td>FIRE</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>thirty</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-11-08 08:01:53-08:00</td>\n",
       "      <td>2018-11-08 08:01:53-08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nine quarters . Affirmative evacuation order i...</td>\n",
       "      <td>78.64</td>\n",
       "      <td>78.85</td>\n",
       "      <td>0.21</td>\n",
       "      <td>spk_0</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.9994, 0.5192, nan, 0.8876, 0.9996, 0.8254, ...</td>\n",
       "      <td>0.82390</td>\n",
       "      <td>0.2566</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "      <td>...</td>\n",
       "      <td>Chico_Paradise_Fire__CalFire</td>\n",
       "      <td>FIRE</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>nine quarters affirmative evacuation order is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-11-08 08:02:18-08:00</td>\n",
       "      <td>2018-11-08 08:02:18-08:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text speaker_start  \\\n",
       "0                      wait FIRE with every night .          52.94   \n",
       "1                                            Thirty          53.39   \n",
       "2  nine quarters . Affirmative evacuation order i...         78.64   \n",
       "\n",
       "  speaker_end speaker_length speaker sentence  \\\n",
       "0       53.39           0.45   spk_0        4   \n",
       "1        53.7           0.31   spk_2        5   \n",
       "2       78.85           0.21   spk_0       12   \n",
       "\n",
       "                                     word_confidence  avg_confidence min_conf  \\\n",
       "0         [0.3086, 0.9876, 1.0, 0.1804, 0.2812, nan]         0.55156   0.1804   \n",
       "1                                           [0.6417]         0.64170   0.6417   \n",
       "2  [0.9994, 0.5192, nan, 0.8876, 0.9996, 0.8254, ...         0.82390   0.2566   \n",
       "\n",
       "                         feed  ...                     feed_name department  \\\n",
       "0  201811081001-444704-22956_  ...  Chico_Paradise_Fire__CalFire       FIRE   \n",
       "1  201811081001-444704-22956_  ...  Chico_Paradise_Fire__CalFire       FIRE   \n",
       "2  201811081001-444704-22956_  ...  Chico_Paradise_Fire__CalFire       FIRE   \n",
       "\n",
       "  INTPTLON INTPTLAT ID_PLACES  \\\n",
       "0       []       []        []   \n",
       "1       []       []        []   \n",
       "2       []       []        []   \n",
       "\n",
       "                                          text_clean contains_fire  \\\n",
       "0                         wait fire with every night             1   \n",
       "1                                             thirty             0   \n",
       "2  nine quarters affirmative evacuation order is ...             0   \n",
       "\n",
       "  contains_evac                start_time                  end_time  \n",
       "0             0 2018-11-08 08:01:52-08:00 2018-11-08 08:01:53-08:00  \n",
       "1             0 2018-11-08 08:01:53-08:00 2018-11-08 08:01:53-08:00  \n",
       "2             1 2018-11-08 08:02:18-08:00 2018-11-08 08:02:18-08:00  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at dataframe\n",
    "sentence.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time   2018-11-08 06:44:31-08:00\n",
      "dtype: datetime64[ns, US/Pacific]\n",
      "start_time   2018-11-08 09:12:25-08:00\n",
      "dtype: datetime64[ns, US/Pacific]\n",
      "end_time   2018-11-08 09:12:26-08:00\n",
      "dtype: datetime64[ns, US/Pacific]\n"
     ]
    }
   ],
   "source": [
    "# Verify results of start and end time to datatime objects\n",
    "print(sentence[['start_time']].min())\n",
    "print(sentence[['start_time']].max())\n",
    "print(sentence[['end_time']].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe by start time\n",
    "sentence.sort_values(by = 'start_time', inplace = True)\n",
    "\n",
    "# Reset index\n",
    "sentence.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Shape:  (464, 22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=464, step=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify changes\n",
    "print('Sentence Shape: ', sentence.shape)\n",
    "sentence.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save words dataframe\n",
    "\n",
    "# Export: save as csv\n",
    "sentence.to_csv('./data/sentence.csv', index=False)\n",
    "\n",
    "# Export: save as pkl\n",
    "sentence.to_pickle('./data/sentence.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection: Threatened Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataframe from the sentence dataframe that only includes the locations mentioned/threatened\n",
    "threat = data_collection.create_threat_df(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>id_places</th>\n",
       "      <th>text</th>\n",
       "      <th>confidence</th>\n",
       "      <th>feed</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.7542</td>\n",
       "      <td>-121.606</td>\n",
       "      <td>PARADISE</td>\n",
       "      <td>justin maguire is clear and counting down from...</td>\n",
       "      <td>0.806101</td>\n",
       "      <td>Oroville_Police_Fire</td>\n",
       "      <td>2018-11-08 06:59:15-08:00</td>\n",
       "      <td>2018-11-08 06:59:17-08:00</td>\n",
       "      <td>BOTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.4829</td>\n",
       "      <td>-118.602</td>\n",
       "      <td>PARADISE</td>\n",
       "      <td>justin maguire is clear and counting down from...</td>\n",
       "      <td>0.806101</td>\n",
       "      <td>Oroville_Police_Fire</td>\n",
       "      <td>2018-11-08 06:59:15-08:00</td>\n",
       "      <td>2018-11-08 06:59:17-08:00</td>\n",
       "      <td>BOTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.4955</td>\n",
       "      <td>-121.56</td>\n",
       "      <td>OROVILLE</td>\n",
       "      <td>left thirty eleven the one you re with and thi...</td>\n",
       "      <td>0.784975</td>\n",
       "      <td>Chico_Paradise_Fire__CalFire</td>\n",
       "      <td>2018-11-08 07:04:34-08:00</td>\n",
       "      <td>2018-11-08 07:04:34-08:00</td>\n",
       "      <td>FIRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.6315</td>\n",
       "      <td>-121.405</td>\n",
       "      <td>BERRY CREEK</td>\n",
       "      <td>left thirty eleven the one you re with and thi...</td>\n",
       "      <td>0.784975</td>\n",
       "      <td>Chico_Paradise_Fire__CalFire</td>\n",
       "      <td>2018-11-08 07:04:34-08:00</td>\n",
       "      <td>2018-11-08 07:04:34-08:00</td>\n",
       "      <td>FIRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.7703</td>\n",
       "      <td>-121.513</td>\n",
       "      <td>CONCOW</td>\n",
       "      <td>left thirty eleven the one you re with and thi...</td>\n",
       "      <td>0.784975</td>\n",
       "      <td>Chico_Paradise_Fire__CalFire</td>\n",
       "      <td>2018-11-08 07:04:34-08:00</td>\n",
       "      <td>2018-11-08 07:04:34-08:00</td>\n",
       "      <td>FIRE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  latitude longitude    id_places  \\\n",
       "0  39.7542  -121.606     PARADISE   \n",
       "1  37.4829  -118.602     PARADISE   \n",
       "2  39.4955   -121.56     OROVILLE   \n",
       "3  39.6315  -121.405  BERRY CREEK   \n",
       "4  39.7703  -121.513       CONCOW   \n",
       "\n",
       "                                                text  confidence  \\\n",
       "0  justin maguire is clear and counting down from...    0.806101   \n",
       "1  justin maguire is clear and counting down from...    0.806101   \n",
       "2  left thirty eleven the one you re with and thi...    0.784975   \n",
       "3  left thirty eleven the one you re with and thi...    0.784975   \n",
       "4  left thirty eleven the one you re with and thi...    0.784975   \n",
       "\n",
       "                           feed                 start_time  \\\n",
       "0          Oroville_Police_Fire  2018-11-08 06:59:15-08:00   \n",
       "1          Oroville_Police_Fire  2018-11-08 06:59:15-08:00   \n",
       "2  Chico_Paradise_Fire__CalFire  2018-11-08 07:04:34-08:00   \n",
       "3  Chico_Paradise_Fire__CalFire  2018-11-08 07:04:34-08:00   \n",
       "4  Chico_Paradise_Fire__CalFire  2018-11-08 07:04:34-08:00   \n",
       "\n",
       "                    end_time department  \n",
       "0  2018-11-08 06:59:17-08:00       BOTH  \n",
       "1  2018-11-08 06:59:17-08:00       BOTH  \n",
       "2  2018-11-08 07:04:34-08:00       FIRE  \n",
       "3  2018-11-08 07:04:34-08:00       FIRE  \n",
       "4  2018-11-08 07:04:34-08:00       FIRE  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export: save as csv\n",
    "threat.to_csv('./data/threat.csv', index=False)\n",
    "\n",
    "# Export: save as pkl\n",
    "threat.to_pickle('./data/threat.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
