{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Police Radio Analysis: Data Collection\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "- [Overview](#Overview)\n",
    "- [Functions](#Functions)\n",
    "- [Dataframe by Word](#Data-Collection:-Single-Word-Observation)\n",
    "- [Dataframe by Sentence](#Data-Collection:-Sentence-by-Speaker-Observation)\n",
    "- [Dataframe by Location (for Mapping)](#Data-Collection:-Threatened-Locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Audio Files: Broadcastify archives for 11/08/2018 between 6:44AM-9:12AM in Butte County, CA (The Camp Fire)\n",
    "\n",
    "\n",
    "- Used Amazon Transcribe to transcribe audio to text (output JSON file)\n",
    "\n",
    "\n",
    "- Steps to convert JSON files to a structured dataframes:\n",
    "    1. Take word and speaker information from individual dataframe to create two dataframes\n",
    "    2. Merge the two dataframes by assigning the speaker to the word based on the time\n",
    "    3. Based on speaker and start/end times reconstruct sentences (new speaker signifies new observation)\n",
    "    4. Repeat for every JSON file and combine all into on dataframe\n",
    "    5. Add additional desired columns based on current information in dataframe:\n",
    "        - Clean version of text (no punctuation, all lowercase)\n",
    "        - Start and stop time as datetime objects using the initial start time in the file name\n",
    "        - Indicator for containing 'fire' or 'evacuation' in the observations\n",
    "    6. Sort dataframe by the start time (datetime object) and export\n",
    "    7. Use existing dataframe and to make a new dataframe where each identified location is one observation\n",
    "\n",
    "\n",
    "- Output: 3 Dataframes\n",
    "    - Dataframe by Individual Words\n",
    "    - Dataframe by Sentences\n",
    "    - Dataframe by Locations Mentioned (for Mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Import Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b9613889427e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import pytz\n",
    "import glob\n",
    "import re\n",
    "from pandas.io.json import json_normalize\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: Requires Docker Environment for GeoPandas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Geographical Data for Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads geographic data from https://data.ca.gov/dataset/ca-geographic-boundaries\n",
    "ca_places = gpd.read_file('./data/CA_places/CA_Places_TIGER2016.shp')\n",
    "ca_state = gpd.read_file('./data/CA_State/CA_State_TIGER2016.shp')\n",
    "ca_counties = gpd.read_file('./data/CA_Counties/CA_Counties_TIGER2016.shp')\n",
    "\n",
    "## Reset INTPTLAT & INTPTLON to floats\n",
    "ca_places['INTPTLAT'] = ca_places['INTPTLAT'].astype('float')\n",
    "ca_places['INTPTLON'] = ca_places['INTPTLON'].astype('float')\n",
    "ca_state['INTPTLAT'] = ca_state['INTPTLAT'].astype('float')\n",
    "ca_state['INTPTLON'] = ca_state['INTPTLON'].astype('float')\n",
    "ca_counties['INTPTLAT'] = ca_counties['INTPTLAT'].astype('float')\n",
    "ca_counties['INTPTLON'] = ca_counties['INTPTLON'].astype('float')\n",
    "\n",
    "## Reset places dataframe to capital NAME\n",
    "for i, row in enumerate(ca_places.iterrows()):\n",
    "    ca_places.loc[i,'NAME'] = row[1]['NAME'].upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 2 dataframes (one for words, one for speakers) from JSON\n",
    "def transcription_outputs(file_name):\n",
    "    \n",
    "    # Read in the JSON file\n",
    "    with open('./' + file_name,'r') as read_file:\n",
    "         data = json.load(read_file)\n",
    "    \n",
    "    # Get Dataframe 1: Words\n",
    "    words = data['results']['items']\n",
    "    words = json_normalize(words,\n",
    "              record_path='alternatives',\n",
    "              meta=['end_time','start_time','type'],\n",
    "              errors='ignore')\n",
    "    words = words[['content','confidence','start_time','end_time','type']]\n",
    "    words['feed'] = data['jobName']\n",
    "\n",
    "    # Get Dataframe 2: Speaker\n",
    "    speaker_labels = data['results']['speaker_labels']\n",
    "    speaker_turns = json_normalize(speaker_labels,\n",
    "                      record_path='segments',\n",
    "                      meta='speakers',\n",
    "                      errors='ignore')\n",
    "    speaker_turns = speaker_turns[['start_time', 'end_time', 'items', 'speaker_label', 'speakers']]\n",
    "    \n",
    "    return words, speaker_turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append speaker to words dataframe based on time\n",
    "def append_speaker(items_df, speakers_df):\n",
    "    \n",
    "    # Set default values\n",
    "    items_df['speaker_start'] = np.nan\n",
    "    items_df['speaker_end'] = np.nan\n",
    "    items_df['sentence'] = np.nan\n",
    "    items_df['speaker'] = np.nan\n",
    "    \n",
    "    # Reset index\n",
    "    items_df.reset_index(inplace=True, drop=True) \n",
    "    \n",
    "    # Format start time in both columns as type float\n",
    "    items_df['start_time'] = items_df['start_time'].astype('float') ### MAY NOT NEED IF ALREADY SET\n",
    "    speakers_df['start_time'] = speakers_df['start_time'].astype('float') ### MAY NOT NEED IF ALREADY SET\n",
    "    \n",
    "    # Iterate through rows in speaker df to append to items_df (dataframe of words)\n",
    "    sentence = 0\n",
    "    for i, sp_row in enumerate(speakers_df.iterrows()):\n",
    "        start_time = sp_row[1][0] ### Make sure index locs are the same\n",
    "        end_time = sp_row[1][1] ### Make sure index locs are the same\n",
    "        speaker = sp_row[1][3] ### Make sure index locs are the same\n",
    "        for i, it_row in enumerate(items_df[items_df['start_time'] >= start_time].iterrows()):\n",
    "            if it_row[1][3] >= end_time: ### Make sure index locs are the same\n",
    "                items_df.loc[it_row[0],'speaker'] = speaker\n",
    "                items_df.loc[it_row[0],'speaker_start'] = start_time\n",
    "                items_df.loc[it_row[0],'speaker_end'] = end_time\n",
    "                items_df.loc[it_row[0],'sentence'] = sentence\n",
    "                sentence += 1\n",
    "                break\n",
    "            else:\n",
    "                items_df.loc[it_row[0],'speaker'] = speaker\n",
    "                items_df.loc[it_row[0],'speaker_start'] = start_time\n",
    "                items_df.loc[it_row[0],'speaker_end'] = end_time\n",
    "                items_df.loc[it_row[0],'sentence'] = sentence\n",
    "    \n",
    "    # Fill in in values for the punctuation rows with the values from the previous text observation\n",
    "    items_df['end_time'] = items_df['end_time'].map(lambda x: np.nan if x is 'nan' else float(x))\n",
    "    items_df['speaker_end'] = items_df['speaker_end'].astype(float)\n",
    "    for column in ['start_time', 'end_time', 'speaker_start', 'speaker_end', 'sentence', 'speaker']:\n",
    "        items_df[column].fillna(method = 'ffill', inplace = True)\n",
    "\n",
    "    # Fix confidence - replace None with np.nan and change values to float (from string)\n",
    "    items_df['confidence'] = items_df['confidence'].map(lambda x: np.nan if x is None else float(x))\n",
    "    \n",
    "    return items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns list of locations from our pre-defined list present in a single observation\n",
    "def get_location(text):\n",
    "    \n",
    "    # Pre-defined location list\n",
    "    location_list = ['CHICO', 'PARADISE', 'OROVILLE', 'MAGALIA', 'THERMALITO', 'GRIDLEY', 'DURHAM', 'PALERMO', 'RIDGE', 'BIGGS', \n",
    "                        'COHASSET', 'BERRY-CREEK', 'FOREST-RANCH', 'BUTTE-CREEK-CANYON', 'BUTTE-VALLEY', 'COHASSET', 'CONCOW', 'BANGOR', \n",
    "                        'HONCUT', 'YANKEE-HILL', 'FORBESTOWN', 'NORD', 'PUGLIA', 'STIRLING-CITY', 'RICHVALE', 'RACKERBY', 'BERRY-CREEK-RANCHERIA', \n",
    "                        'CLIPPER-MILLS', 'ROBINSON-MILL', 'CHEROKEE', 'BUTTE-MEADOWS', 'ENTERPRISE-RANCHERIA']\n",
    "    \n",
    "    # Get list of locatoins mentioned\n",
    "    locations_mentioned = []\n",
    "    for location in location_list:\n",
    "        if location.lower() in text.lower():\n",
    "            locations_mentioned.append(location.replace('-',' '))\n",
    "    \n",
    "    return locations_mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps a list of the potential location matches from the CA_places dataframe and appends lat & long data\n",
    "def label_centroids(df, ca_places):\n",
    "    df['INTPTLON'] = 'None'\n",
    "    df['INTPTLAT'] = 'None'\n",
    "    df['ID_PLACES'] = 'None'\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        intptlon = []\n",
    "        intptlat = []\n",
    "        id_places = []\n",
    "        for loc in row[1]['location']:\n",
    "            if loc in ca_places['NAME'].tolist():\n",
    "                intptlon.extend(ca_places[ca_places['NAME'] == loc]['INTPTLON'])\n",
    "                intptlat.extend(ca_places[ca_places['NAME'] == loc]['INTPTLAT'])\n",
    "                id_places.extend(ca_places[ca_places['NAME'] == loc]['NAME'])\n",
    "        df.at[i,'INTPTLON'] = intptlon\n",
    "        df.at[i,'INTPTLAT'] = intptlat\n",
    "        df.at[i,'ID_PLACES'] = id_places\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps file name to actual feed names (5 different feeds used)\n",
    "def remap_feed(filename_str):\n",
    "    \n",
    "    # Dictionary to map feed numbers to feed bames\n",
    "    feeds = {\n",
    "        '1929'  : \"Butte_Sheriff_Fire__Paradise_Police\",\n",
    "        '22956' : \"Chico_Paradise_Fire__CalFire\",\n",
    "        '25641' : \"Chico_Police_Dispatch\",\n",
    "        '24574' : \"Oroville_Fire\",\n",
    "        '26936' : \"Oroville_Police_Fire\"\n",
    "    }\n",
    "    f = filename_str\n",
    "    code = f[f.rfind('-')+1:-1]\n",
    "    \n",
    "    return feeds[code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate which department the feed is associated with (Fire or Police)\n",
    "def police_fire(feed_name):\n",
    "    \n",
    "    # Feed name to uppercase\n",
    "    fnu = feed_name.upper()\n",
    "    \n",
    "    # Look for 'Fire' and 'Police' in feed name\n",
    "    if 'FIRE' in fnu and 'POLICE' in fnu:\n",
    "        return 'BOTH'\n",
    "    elif 'FIRE' in fnu:\n",
    "        return 'FIRE'\n",
    "    elif 'POLICE' in fnu:\n",
    "        return 'POLICE'\n",
    "    else:\n",
    "        return 'FEED_NAME_ERROR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes start time to the actual time (datetime object) based on thes start time of the entire feed\n",
    "def actual_time_str(timecode_str, filename_str):\n",
    "    \n",
    "    filename_str = filename_str.split('/')[-1]\n",
    "\n",
    "    # Get year, month, day, hour and minutes from file name\n",
    "    YYYY = filename_str[:4]\n",
    "    MM = filename_str[4:6]\n",
    "    DD = filename_str[6:8]\n",
    "    hh = filename_str[8:10]\n",
    "    mm = filename_str[10:12]\n",
    "    ssssmmmm  = float(timecode_str) # added via timedelta\n",
    "    \n",
    "    # adjust hours (file name is 2 hours behind actual time)\n",
    "    hh_adj = str(int(hh)-2).zfill(2)\n",
    "   \n",
    "    # Create date time object \n",
    "    dt_str = ''.join([YYYY,MM,DD,hh_adj,mm])\n",
    "    dt_naive = datetime.strptime(dt_str, '%Y%m%d%H%M')\n",
    "    tz_pac = pytz.timezone('US/Pacific')\n",
    "    dt_pac_0 = tz_pac.localize(dt_naive)\n",
    "    dt_pac_actual = dt_pac_0 + timedelta(seconds = int(ssssmmmm))\n",
    "                                     \n",
    "    return dt_pac_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take individual words and reconstruct into a sentence based on the time and speaker\n",
    "def sentence_reconstruction(items_spkr_appnd, ca_places):\n",
    "    \n",
    "    # Create empty dataframe with desired columns\n",
    "    df = pd.DataFrame(data='', index=[0], columns=['text','speaker_start','speaker_end','speaker_length','speaker', \n",
    "                                                   'sentence', 'word_confidence','avg_confidence','min_conf','feed'])\n",
    "\n",
    "    # Iterate through items dataframe to construct sentences\n",
    "    index_set = 0\n",
    "    confidence = []\n",
    "    text = ''\n",
    "    previous_speaker = items_spkr_appnd['speaker'][0]\n",
    "    for i, speaker in enumerate(items_spkr_appnd['speaker']):\n",
    "        if speaker != previous_speaker:\n",
    "            df.loc[index_set,:] = {\n",
    "                'speaker_start' : items_spkr_appnd.loc[i,'speaker_start'],\n",
    "                'speaker_end' : items_spkr_appnd.loc[i,'speaker_end'],\n",
    "                'speaker' : items_spkr_appnd.loc[i,'speaker'],\n",
    "                'sentence' : items_spkr_appnd.loc[i,'sentence'],\n",
    "                'text' : text,\n",
    "                'word_confidence' : confidence,\n",
    "                'min_conf' : min(confidence),\n",
    "                'feed' : items_spkr_appnd.loc[i,'feed']\n",
    "            }\n",
    "            index_set += 1\n",
    "            text = ''\n",
    "            confidence = []\n",
    "        text += str(items_spkr_appnd.loc[i, 'content'] + ' ')\n",
    "        confidence.append(items_spkr_appnd.loc[i,'confidence'])\n",
    "        previous_speaker = speaker\n",
    "    \n",
    "    # Calculate average confidence by removing NAs\n",
    "    df['avg_confidence'] = df['word_confidence'].map(lambda list: np.mean([x for x in list if str(x) != 'nan']))\n",
    "    \n",
    "    # Add speaker_length\n",
    "    df['speaker_length'] = df['speaker_end'] - df['speaker_start']\n",
    "    \n",
    "    # Add new columns: location, state, feed_name and is_fire_dept\n",
    "    df['location'] = df['text'].map(get_location)\n",
    "    df['state'] = 'CA'\n",
    "    df['feed_name'] = df['feed'].map(remap_feed)\n",
    "    df['department'] = df['feed_name'].map(police_fire)\n",
    "    df = label_centroids(df, ca_places)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilize previous functions to get master dataframe in the desired format\n",
    "def get_dataframe(file_name, ca_places):\n",
    "    \n",
    "    # File name\n",
    "    file_name = file_name\n",
    "\n",
    "    # Step 1: get items and speaker df from json\n",
    "    items_df, speakers_df = transcription_outputs(file_name)\n",
    "                                                     \n",
    "    # Step 2: combine dataframes (single word observations with speaker)\n",
    "    df = append_speaker(items_df, speakers_df)\n",
    "\n",
    "    # Step 3: observations by sentence and additional desired columns\n",
    "    df = sentence_reconstruction(df, ca_places)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe based on location to be used for location mapping\n",
    "def create_threat_df(df):\n",
    "    threat_df = pd.DataFrame(columns=['latitude','longitude'])\n",
    "    index = 0\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        if len(row[1]['INTPTLAT']) > 0:\n",
    "            for j, loc in enumerate(row[1]['INTPTLAT']):\n",
    "                threat_df.loc[index,'latitude'] = row[1]['INTPTLAT'][j]\n",
    "                threat_df.loc[index,'longitude'] = row[1]['INTPTLON'][j]\n",
    "                threat_df.loc[index,'id_places'] = row[1]['ID_PLACES'][j]\n",
    "                threat_df.loc[index,'text'] = row[1]['text_clean']\n",
    "                threat_df.loc[index,'confidence'] = row[1]['avg_confidence']\n",
    "                threat_df.loc[index,'feed'] = row[1]['feed_name']\n",
    "                threat_df.loc[index,'start_time'] = row[1]['start_time']\n",
    "                threat_df.loc[index,'end_time'] = row[1]['end_time']\n",
    "                threat_df.loc[index,'department'] = row[1]['department']\n",
    "                index += 1\n",
    "    return threat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection: Single Word Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 20 Dataframes Completed (558 rows): ./data/translations/201811081001-444704-22956_.json\n",
      "2 of 20 Dataframes Completed (1146 rows): ./data/translations/201811080929-467022-25641_.json\n",
      "3 of 20 Dataframes Completed (1201 rows): ./data/translations/201811080858-659667-24574_.json\n",
      "4 of 20 Dataframes Completed (2511 rows): ./data/translations/201811080931-763045-22956_.json\n",
      "5 of 20 Dataframes Completed (3921 rows): ./data/translations/201811080901-584135-22956_.json\n",
      "6 of 20 Dataframes Completed (4113 rows): ./data/translations/201811081011-319947-26936_.json\n",
      "7 of 20 Dataframes Completed (4253 rows): ./data/translations/201811080911-136992-26936_.json\n",
      "8 of 20 Dataframes Completed (5080 rows): ./data/translations/201811081012-237044-1929_.json\n",
      "9 of 20 Dataframes Completed (5143 rows): ./data/translations/201811080959-402082-25641_.json\n",
      "10 of 20 Dataframes Completed (5208 rows): ./data/translations/201811080928-650127-24574_.json\n",
      "11 of 20 Dataframes Completed (5637 rows): ./data/translations/201811081031-942894-22956_.json\n",
      "12 of 20 Dataframes Completed (6349 rows): ./data/translations/201811080942-79066-1929_.json\n",
      "13 of 20 Dataframes Completed (6562 rows): ./data/translations/201811081027-123435-24574_.json\n",
      "14 of 20 Dataframes Completed (7440 rows): ./data/translations/201811081042-434498-1929_.json\n",
      "15 of 20 Dataframes Completed (7665 rows): ./data/translations/201811080841-581016-26936_.json\n",
      "16 of 20 Dataframes Completed (8229 rows): ./data/translations/201811081029-313400-25641_.json\n",
      "17 of 20 Dataframes Completed (8313 rows): ./data/translations/201811080941-981417-26936_.json\n",
      "18 of 20 Dataframes Completed (9132 rows): ./data/translations/201811080913-878163-1929_.json\n",
      "19 of 20 Dataframes Completed (9257 rows): ./data/translations/201811080958-305482-24574_.json\n",
      "20 of 20 Dataframes Completed (9644 rows): ./data/translations/201811080900-205450-25641_.json\n",
      "Words Dataframe Shape: (9644, 6)\n"
     ]
    }
   ],
   "source": [
    "# Get word dataframe\n",
    "\n",
    "# Use glob to get list of all json files in the folder\n",
    "files_json = (glob.glob('./data/translations/*.json'))\n",
    "\n",
    "# Create empty dataframe\n",
    "words = pd.DataFrame()\n",
    "rows = 0 \n",
    "\n",
    "for i, file_name in enumerate(files_json):\n",
    "    word_one_file, _ = transcription_outputs(file_name)\n",
    "    \n",
    "    rows += len(word_one_file)\n",
    "    \n",
    "    # Print status\n",
    "    print(f'{i+1} of {len(files_json)} Dataframes Completed ({rows} rows): {file_name}')\n",
    "    \n",
    "    # Add each dataframe together\n",
    "    words = pd.concat([words, word_one_file])\n",
    "    \n",
    "# Reset index of master dataframe\n",
    "words.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print shape of dataframe after going through all JSON files\n",
    "print(f'Words Dataframe Shape: {words.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>confidence</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>type</th>\n",
       "      <th>feed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wait</td>\n",
       "      <td>0.3086</td>\n",
       "      <td>38.04</td>\n",
       "      <td>39.28</td>\n",
       "      <td>pronunciation</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FIRE</td>\n",
       "      <td>0.9876</td>\n",
       "      <td>39.29</td>\n",
       "      <td>39.88</td>\n",
       "      <td>pronunciation</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>with</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.88</td>\n",
       "      <td>40.64</td>\n",
       "      <td>pronunciation</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>every</td>\n",
       "      <td>0.1804</td>\n",
       "      <td>41.22</td>\n",
       "      <td>41.41</td>\n",
       "      <td>pronunciation</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>night</td>\n",
       "      <td>0.2812</td>\n",
       "      <td>41.41</td>\n",
       "      <td>41.72</td>\n",
       "      <td>pronunciation</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  content confidence start_time end_time           type  \\\n",
       "0    wait     0.3086      38.04    39.28  pronunciation   \n",
       "1    FIRE     0.9876      39.29    39.88  pronunciation   \n",
       "2    with        1.0      39.88    40.64  pronunciation   \n",
       "3   every     0.1804      41.22    41.41  pronunciation   \n",
       "4   night     0.2812      41.41    41.72  pronunciation   \n",
       "\n",
       "                         feed  \n",
       "0  201811081001-444704-22956_  \n",
       "1  201811081001-444704-22956_  \n",
       "2  201811081001-444704-22956_  \n",
       "3  201811081001-444704-22956_  \n",
       "4  201811081001-444704-22956_  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save words dataframe\n",
    "\n",
    "# Export: save as csv\n",
    "words.to_csv('./data/words.csv', index=False)\n",
    "\n",
    "# Export: save as pkl\n",
    "words.to_pickle('./data/words.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection: Sentence by Speaker Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 20 Dataframes Completed (new rows: 52, total rows: 52): ./data/translations/201811081001-444704-22956_.json\n",
      "2 of 20 Dataframes Completed (new rows: 44, total rows: 96): ./data/translations/201811080929-467022-25641_.json\n",
      "3 of 20 Dataframes Completed (new rows: 6, total rows: 102): ./data/translations/201811080858-659667-24574_.json\n",
      "4 of 20 Dataframes Completed (new rows: 50, total rows: 152): ./data/translations/201811080931-763045-22956_.json\n",
      "5 of 20 Dataframes Completed (new rows: 30, total rows: 182): ./data/translations/201811080901-584135-22956_.json\n",
      "6 of 20 Dataframes Completed (new rows: 10, total rows: 192): ./data/translations/201811081011-319947-26936_.json\n",
      "7 of 20 Dataframes Completed (new rows: 7, total rows: 199): ./data/translations/201811080911-136992-26936_.json\n",
      "8 of 20 Dataframes Completed (new rows: 45, total rows: 244): ./data/translations/201811081012-237044-1929_.json\n",
      "9 of 20 Dataframes Completed (new rows: 2, total rows: 246): ./data/translations/201811080959-402082-25641_.json\n",
      "10 of 20 Dataframes Completed (new rows: 3, total rows: 249): ./data/translations/201811080928-650127-24574_.json\n",
      "11 of 20 Dataframes Completed (new rows: 36, total rows: 285): ./data/translations/201811081031-942894-22956_.json\n",
      "12 of 20 Dataframes Completed (new rows: 21, total rows: 306): ./data/translations/201811080942-79066-1929_.json\n",
      "13 of 20 Dataframes Completed (new rows: 20, total rows: 326): ./data/translations/201811081027-123435-24574_.json\n",
      "14 of 20 Dataframes Completed (new rows: 30, total rows: 356): ./data/translations/201811081042-434498-1929_.json\n",
      "15 of 20 Dataframes Completed (new rows: 13, total rows: 369): ./data/translations/201811080841-581016-26936_.json\n",
      "16 of 20 Dataframes Completed (new rows: 21, total rows: 390): ./data/translations/201811081029-313400-25641_.json\n",
      "17 of 20 Dataframes Completed (new rows: 3, total rows: 393): ./data/translations/201811080941-981417-26936_.json\n",
      "18 of 20 Dataframes Completed (new rows: 38, total rows: 431): ./data/translations/201811080913-878163-1929_.json\n",
      "19 of 20 Dataframes Completed (new rows: 16, total rows: 447): ./data/translations/201811080958-305482-24574_.json\n",
      "20 of 20 Dataframes Completed (new rows: 17, total rows: 464): ./data/translations/201811080900-205450-25641_.json\n",
      "Sentence Dataframe Shape: (464, 17)\n"
     ]
    }
   ],
   "source": [
    "# Get sentence dataframe\n",
    "\n",
    "# Use glob to get list of all json files in the folder\n",
    "files_json = (glob.glob('./data/translations/*.json'))\n",
    "\n",
    "# Create empty dataframe\n",
    "sentence = pd.DataFrame()\n",
    "rows = 0 \n",
    "\n",
    "# Iterate through the files and get a dataframe for each file\n",
    "for i, file_name in enumerate(files_json):\n",
    "    \n",
    "    # Get dataframe for an individual file\n",
    "    df_one_file = get_dataframe(file_name, ca_places)\n",
    "    rows += len(df_one_file)\n",
    "    \n",
    "    # Print status\n",
    "    print(f'{i+1} of {len(files_json)} Dataframes Completed (new rows: {len(df_one_file)}, total rows: {rows}): {file_name}')\n",
    "    \n",
    "    # Add each dataframe together\n",
    "    sentence = pd.concat([sentence, df_one_file])\n",
    "\n",
    "# Reset index of master dataframe\n",
    "sentence.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print shape of dataframe after going through all JSON files\n",
    "print(f'Sentence Dataframe Shape: {sentence.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Additional Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for clean text (remove punctuation and make all lowercase)\n",
    "# Reference: Code adapted from NLP_EDA-InClass in DEN Flex by Sam Stack\n",
    "def clean_text(raw_text):\n",
    "    words = re.sub(r'[^a-z0-9]', r' ', raw_text.lower()).split()\n",
    "    return ' '.join(words)\n",
    "\n",
    "sentence['text_clean'] = sentence['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for start and end time as datetime objects (using speaker_start/end and file name)\n",
    "start_time = []\n",
    "end_time = []\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    start_time.append(actual_time_str(sentence['speaker_start'][i], sentence['feed'][i]))\n",
    "    end_time.append(actual_time_str(sentence['speaker_end'][i], sentence['feed'][i]))\n",
    "\n",
    "sentence['start_time'] = start_time\n",
    "sentence['end_time'] = end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns that indicate if fire or evacuation related words were mentioned in that observation\n",
    "sentence['contains_fire'] = sentence['text_clean'].map(lambda x: 1 if 'fire' in x else 0)\n",
    "sentence['contains_evac'] = sentence['text_clean'].map(lambda x: 1 if 'evac' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>speaker_start</th>\n",
       "      <th>speaker_end</th>\n",
       "      <th>speaker_length</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_confidence</th>\n",
       "      <th>avg_confidence</th>\n",
       "      <th>min_conf</th>\n",
       "      <th>feed</th>\n",
       "      <th>...</th>\n",
       "      <th>feed_name</th>\n",
       "      <th>department</th>\n",
       "      <th>INTPTLON</th>\n",
       "      <th>INTPTLAT</th>\n",
       "      <th>ID_PLACES</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>contains_fire</th>\n",
       "      <th>contains_evac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wait FIRE with every night .</td>\n",
       "      <td>52.94</td>\n",
       "      <td>53.39</td>\n",
       "      <td>0.45</td>\n",
       "      <td>spk_0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.3086, 0.9876, 1.0, 0.1804, 0.2812, nan]</td>\n",
       "      <td>0.55156</td>\n",
       "      <td>0.1804</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "      <td>...</td>\n",
       "      <td>Chico_Paradise_Fire__CalFire</td>\n",
       "      <td>FIRE</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>wait fire with every night</td>\n",
       "      <td>2018-11-08 08:01:52-08:00</td>\n",
       "      <td>2018-11-08 08:01:53-08:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thirty</td>\n",
       "      <td>53.39</td>\n",
       "      <td>53.7</td>\n",
       "      <td>0.31</td>\n",
       "      <td>spk_2</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.6417]</td>\n",
       "      <td>0.64170</td>\n",
       "      <td>0.6417</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "      <td>...</td>\n",
       "      <td>Chico_Paradise_Fire__CalFire</td>\n",
       "      <td>FIRE</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>thirty</td>\n",
       "      <td>2018-11-08 08:01:53-08:00</td>\n",
       "      <td>2018-11-08 08:01:53-08:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nine quarters . Affirmative evacuation order i...</td>\n",
       "      <td>78.64</td>\n",
       "      <td>78.85</td>\n",
       "      <td>0.21</td>\n",
       "      <td>spk_0</td>\n",
       "      <td>12</td>\n",
       "      <td>[0.9994, 0.5192, nan, 0.8876, 0.9996, 0.8254, ...</td>\n",
       "      <td>0.82390</td>\n",
       "      <td>0.2566</td>\n",
       "      <td>201811081001-444704-22956_</td>\n",
       "      <td>...</td>\n",
       "      <td>Chico_Paradise_Fire__CalFire</td>\n",
       "      <td>FIRE</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>nine quarters affirmative evacuation order is ...</td>\n",
       "      <td>2018-11-08 08:02:18-08:00</td>\n",
       "      <td>2018-11-08 08:02:18-08:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text speaker_start  \\\n",
       "0                      wait FIRE with every night .          52.94   \n",
       "1                                            Thirty          53.39   \n",
       "2  nine quarters . Affirmative evacuation order i...         78.64   \n",
       "\n",
       "  speaker_end speaker_length speaker sentence  \\\n",
       "0       53.39           0.45   spk_0        4   \n",
       "1        53.7           0.31   spk_2        5   \n",
       "2       78.85           0.21   spk_0       12   \n",
       "\n",
       "                                     word_confidence  avg_confidence min_conf  \\\n",
       "0         [0.3086, 0.9876, 1.0, 0.1804, 0.2812, nan]         0.55156   0.1804   \n",
       "1                                           [0.6417]         0.64170   0.6417   \n",
       "2  [0.9994, 0.5192, nan, 0.8876, 0.9996, 0.8254, ...         0.82390   0.2566   \n",
       "\n",
       "                         feed  ...                     feed_name department  \\\n",
       "0  201811081001-444704-22956_  ...  Chico_Paradise_Fire__CalFire       FIRE   \n",
       "1  201811081001-444704-22956_  ...  Chico_Paradise_Fire__CalFire       FIRE   \n",
       "2  201811081001-444704-22956_  ...  Chico_Paradise_Fire__CalFire       FIRE   \n",
       "\n",
       "  INTPTLON INTPTLAT ID_PLACES  \\\n",
       "0       []       []        []   \n",
       "1       []       []        []   \n",
       "2       []       []        []   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                         wait fire with every night   \n",
       "1                                             thirty   \n",
       "2  nine quarters affirmative evacuation order is ...   \n",
       "\n",
       "                 start_time                  end_time contains_fire  \\\n",
       "0 2018-11-08 08:01:52-08:00 2018-11-08 08:01:53-08:00             1   \n",
       "1 2018-11-08 08:01:53-08:00 2018-11-08 08:01:53-08:00             0   \n",
       "2 2018-11-08 08:02:18-08:00 2018-11-08 08:02:18-08:00             0   \n",
       "\n",
       "  contains_evac  \n",
       "0             0  \n",
       "1             0  \n",
       "2             1  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at dataframe\n",
    "sentence.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time   2018-11-08 06:44:31-08:00\n",
      "dtype: datetime64[ns, US/Pacific]\n",
      "start_time   2018-11-08 09:12:25-08:00\n",
      "dtype: datetime64[ns, US/Pacific]\n",
      "end_time   2018-11-08 09:12:26-08:00\n",
      "dtype: datetime64[ns, US/Pacific]\n"
     ]
    }
   ],
   "source": [
    "# Verify results of start and end time to datatime objects\n",
    "print(sentence[['start_time']].min())\n",
    "print(sentence[['start_time']].max())\n",
    "print(sentence[['end_time']].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe by start time\n",
    "sentence.sort_values(by = 'start_time', inplace = True)\n",
    "\n",
    "# Reset index\n",
    "sentence.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Shape:  (464, 22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=464, step=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify changes\n",
    "print('Sentence Shape: ', sentence.shape)\n",
    "sentence.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save words dataframe\n",
    "\n",
    "# Export: save as csv\n",
    "sentence.to_csv('./data/sentence.csv', index=False)\n",
    "\n",
    "# Export: save as pkl\n",
    "sentence.to_pickle('./data/sentence.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection: Threatened Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataframe from the sentence dataframe that only includes the locations mentioned/threatened\n",
    "threat = create_threat_df(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>id_places</th>\n",
       "      <th>text</th>\n",
       "      <th>confidence</th>\n",
       "      <th>feed</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.7542</td>\n",
       "      <td>-121.606</td>\n",
       "      <td>PARADISE</td>\n",
       "      <td>justin maguire is clear and counting down from...</td>\n",
       "      <td>0.806101</td>\n",
       "      <td>Oroville_Police_Fire</td>\n",
       "      <td>2018-11-08 06:59:15-08:00</td>\n",
       "      <td>2018-11-08 06:59:17-08:00</td>\n",
       "      <td>BOTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.4829</td>\n",
       "      <td>-118.602</td>\n",
       "      <td>PARADISE</td>\n",
       "      <td>justin maguire is clear and counting down from...</td>\n",
       "      <td>0.806101</td>\n",
       "      <td>Oroville_Police_Fire</td>\n",
       "      <td>2018-11-08 06:59:15-08:00</td>\n",
       "      <td>2018-11-08 06:59:17-08:00</td>\n",
       "      <td>BOTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.4955</td>\n",
       "      <td>-121.56</td>\n",
       "      <td>OROVILLE</td>\n",
       "      <td>left thirty eleven the one you re with and thi...</td>\n",
       "      <td>0.784975</td>\n",
       "      <td>Chico_Paradise_Fire__CalFire</td>\n",
       "      <td>2018-11-08 07:04:34-08:00</td>\n",
       "      <td>2018-11-08 07:04:34-08:00</td>\n",
       "      <td>FIRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39.6315</td>\n",
       "      <td>-121.405</td>\n",
       "      <td>BERRY CREEK</td>\n",
       "      <td>left thirty eleven the one you re with and thi...</td>\n",
       "      <td>0.784975</td>\n",
       "      <td>Chico_Paradise_Fire__CalFire</td>\n",
       "      <td>2018-11-08 07:04:34-08:00</td>\n",
       "      <td>2018-11-08 07:04:34-08:00</td>\n",
       "      <td>FIRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.7703</td>\n",
       "      <td>-121.513</td>\n",
       "      <td>CONCOW</td>\n",
       "      <td>left thirty eleven the one you re with and thi...</td>\n",
       "      <td>0.784975</td>\n",
       "      <td>Chico_Paradise_Fire__CalFire</td>\n",
       "      <td>2018-11-08 07:04:34-08:00</td>\n",
       "      <td>2018-11-08 07:04:34-08:00</td>\n",
       "      <td>FIRE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  latitude longitude    id_places  \\\n",
       "0  39.7542  -121.606     PARADISE   \n",
       "1  37.4829  -118.602     PARADISE   \n",
       "2  39.4955   -121.56     OROVILLE   \n",
       "3  39.6315  -121.405  BERRY CREEK   \n",
       "4  39.7703  -121.513       CONCOW   \n",
       "\n",
       "                                                text  confidence  \\\n",
       "0  justin maguire is clear and counting down from...    0.806101   \n",
       "1  justin maguire is clear and counting down from...    0.806101   \n",
       "2  left thirty eleven the one you re with and thi...    0.784975   \n",
       "3  left thirty eleven the one you re with and thi...    0.784975   \n",
       "4  left thirty eleven the one you re with and thi...    0.784975   \n",
       "\n",
       "                           feed                 start_time  \\\n",
       "0          Oroville_Police_Fire  2018-11-08 06:59:15-08:00   \n",
       "1          Oroville_Police_Fire  2018-11-08 06:59:15-08:00   \n",
       "2  Chico_Paradise_Fire__CalFire  2018-11-08 07:04:34-08:00   \n",
       "3  Chico_Paradise_Fire__CalFire  2018-11-08 07:04:34-08:00   \n",
       "4  Chico_Paradise_Fire__CalFire  2018-11-08 07:04:34-08:00   \n",
       "\n",
       "                    end_time department  \n",
       "0  2018-11-08 06:59:17-08:00       BOTH  \n",
       "1  2018-11-08 06:59:17-08:00       BOTH  \n",
       "2  2018-11-08 07:04:34-08:00       FIRE  \n",
       "3  2018-11-08 07:04:34-08:00       FIRE  \n",
       "4  2018-11-08 07:04:34-08:00       FIRE  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export: save as csv\n",
    "threat.to_csv('./data/threat.csv', index=False)\n",
    "\n",
    "# Export: save as pkl\n",
    "threat.to_pickle('./data/threat.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
